# -*- coding: utf-8 -*-
"""ML_Demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mSladZBswjXVaJz-yGJzyTS3AdmbjAKj

#WOMANIUM 2024 Project Demo

#Supervised Machine Learning Model

We use the code from [Jarvis Tools Notebooks](https://github.com/JARVIS-Materials-Design/jarvis-tools-notebooks?tab=readme-ov-file#intro) and this [tutorial](https://forecastegy.com/posts/lightgbm-binary-classification-python/) to train and test our model and retrieve our dataset. This model is a binary classifier that will help us decide whether a material is potentially suitable or not for PV cells. The criteria for creating the dataset was based on the paper on [Accelerated Discovery of Efficient Solar Cell Materials Using Quantum and Machine-Learning Methods](https://pubs.acs.org/doi/full/10.1021/acs.chemmater.9b02166)

## Preparing the data
"""

!pip install jarvis-tools

from jarvis.db.figshare import data #retrieve data of materials
import pandas as pd #for data manipulation and creating dataframes which are like tables
df=pd.DataFrame(data('dft_3d')) #3d dataset includes geometric and visual properties

#Based on the Accelerated Discovery of Efficient Solar Cell Materials Using Quantum and Machine Learning Methods, we are interested in materials with SLME values > 10%
#remove the data that does not have SLME values
df_slme = df[df['slme'] != 'na'].copy()
#to compare the slme values we need to convert it to a float
df_slme.loc[:,'slme'] = df_slme['slme'].astype(float)
#add a new column slme_10 that has a 1 if slme value >=10.0 otherwise 0
df_slme.loc[:,'slme_10'] = (df_slme['slme'] >= 10.0).astype(int)
df_slme #display the data

#Drop the columns that are not categorical or numerical such as IDs or arrays
df_slme = df_slme.drop(columns=['jid', 'atoms', 'elastic_tensor', 'effective_masses_300K', 'modes', 'icsd','efg', 'xml_data_link', 'raw_files', 'reference', 'search'])

#replace na with NaN
import numpy as np
df_slme.replace('na', np.nan,  inplace=True)

#convert columns as int/float/boolean as appropriate. This is important for when we train our model

col_convert_float = [ 'magmom_oszicar', 'spillage', 'maxdiff_mesh', 'maxdiff_bz', 'epsx', 'epsy', 'epsz',
    'mepsx', 'mepsy', 'mepsz', 'magmom_outcar', 'max_efg', 'avg_elec_mass', 'avg_hole_mass',
    'dfpt_piezo_max_eij', 'dfpt_piezo_max_dij', 'dfpt_piezo_max_dielectric',
    'dfpt_piezo_max_dielectric_electronic', 'dfpt_piezo_max_dielectric_ionic',
    'max_ir_mode', 'min_ir_mode', 'n-Seebeck', 'p-Seebeck', 'n-powerfact', 'p-powerfact',
    'ncond', 'pcond', 'nkappa', 'pkappa', 'Tc_supercon', 'exfoliation_energy', 'poisson',
    'bulk_modulus_kv', 'shear_modulus_gv', 'mbj_bandgap', 'hse_gap']

for col_name in col_convert_float:
    df_slme.loc[:,col_name] = df_slme[col_name].astype(float)


df_slme.loc[:,'encut'] = df_slme['encut'].fillna(0).astype(int)
df_slme.loc[:,'kpoint_length_unit'] = df_slme['kpoint_length_unit'].fillna(0).astype(int)

#display table to see table with cleaned values
df_slme

#this code was used to investigate the values in each column
#pd.set_option('display.max_rows',None)
#print(df_slme['search'])

# Commented out IPython magic to ensure Python compatibility.
#check if the data is imbalanced this means if there are more 1s and 0s or vice versa
import matplotlib.pyplot as plt
# %matplotlib inline
check_imbalance = df_slme['slme_10'].value_counts().reset_index()
check_imbalance.columns = ['slme_10', 'count']

# Create the bar plot using matplotlib
plt.bar(check_imbalance['slme_10'], check_imbalance['count'])
plt.xlabel('slme_10')
plt.ylabel('Count')
plt.xticks(check_imbalance['slme_10'])
plt.show()

"""# Split the data into training and test"""

from sklearn.model_selection import train_test_split
import random
from datetime import datetime
#features is what the model will use to predict the target variable, slme_10
features = df_slme.drop(columns=['slme_10','slme', 'formula'])
target = df_slme['slme_10']

#test dataset size is 20% and traing dataset is 80% (random_state is the seed so that it chooses random materials to put in training nad testing data set)
random.seed(datetime.now().timestamp())
random_num = random.randint(1,100)
features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=random_num)

#get the indices of the test values so we can retrieve the material's formula later
test_indices = features_test.index

#make sure that we don't train with slme_10 and slme bychecking the columns
features.columns

"""## Prepocess the data"""

!pip install category_encoders

#convert categorical data into integers
import category_encoders as ce
encoder = ce.OrdinalEncoder(cols=['spg_number', 'spg_symbol', 'func', 'dimensionality', 'spg', 'crys', 'typ'])

features_train = encoder.fit_transform(features_train)
features_test = encoder.fit_transform(features_test)

#make sure that there we raplace missing values with NaNs
features_train = features_train.replace('?', np.nan)
features_test = features_test.replace('?', np.nan)

"""## Setting the classfier model hyperparameters and training"""

from lightgbm import LGBMClassifier
# learning rate = how much each tree contributes to the overall predicition. The lower learning rate is better but takes longer training time
#n_estimators = number of trees to build. Higher value means longer training time
#num_leaves= number of leaves in each tree. Higher value means a less regularized model which can lead to overfitting

#hyperparameters from the paper mentioned above
model = LGBMClassifier(learning_rate=0.01, n_estimators=5000, num_leaves=100, max_depth=50, boosting_type='gbdt', min_child_samples = 20, min_child_weight = 0.001, min_split_gain=0.0, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0, subsample_for_bin=200000,subsample_freq=1)

model.fit(features_train, target_train, categorical_feature=['spg_number', 'spg_symbol', 'func', 'dimensionality', 'spg', 'crys', 'typ'] )

"""## Make predictions"""

#this directly predicts the class for each instance in the test set. the threshold of 0.5 fpr predicted probabilities
pred = model.predict(features_test)

#these are predicted probabilities rather than class predictions
pred_probabilities = model.predict_proba(features_test)[:,1]

#create dataframe to show which materials are suitable or not. This is mapping formulas back to the indices we save earlier

predictions_df = pd.DataFrame({ 'formula' : df_slme.loc[test_indices, 'formula'],
                               'predictions': pred })
print(predictions_df)
print("number of test values predicted: ", len(predictions_df))
print("number of potential materials: ", predictions_df['predictions'].sum())

"""# Evaluate the model

Note that the following values used to evaluate the model will vary every time we run all of the cells above because we choose random values for what is included in our testing and training dataset. However, the values should be similar.
"""

#accuracy is one metric. It is the proportion of predictions that the mode got right. Number closer to 1 is good
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(target_test, pred)
print("Accuracy", accuracy )

#log loss is another metric that evaluates the predicted probabilities of membership to a given class. Number closer to 0 is good
from sklearn.metrics import log_loss
loss = log_loss(target_test, pred_probabilities)
print("Log Loss", loss)

#ROC AUC (Area under the ROC curve- Receiver operating characteristic curve: this curve plots the possible true positive rates agains false positive rates) score
#it is an aggregate measure of performance across all possible classification thresholds. Number closer to 1 is good
from sklearn.metrics import roc_auc_score
roc = roc_auc_score(target_test, pred)
print("ROC AUC ", roc)

#general classification report
from sklearn.metrics import classification_report
report = classification_report(target_test, pred)
print("Classification Report")
print(report)

#confusion matrix to summarize performance of a classification model on a set of data where true values are known
from sklearn.metrics import ConfusionMatrixDisplay
matrix = ConfusionMatrixDisplay.from_predictions(target_test,pred)
plt.show()

